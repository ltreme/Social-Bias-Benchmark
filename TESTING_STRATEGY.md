# Testing Strategy fÃ¼r Social Bias Benchmark

## Ãœberblick

Dieses Dokument beschreibt eine umfassende Teststrategie fÃ¼r den Social Bias Benchmark, um sicherzustellen, dass teure GPU-Server-Runs fehlerfrei ablaufen.

## Kritische Komponenten (nach Risiko sortiert)

### ðŸ”´ KRITISCH - Hohe PrioritÃ¤t

#### 1. **LLM-Integration & Prompt-Generierung**
- **Komponenten:**
  - `LlmClientVLLMBench`, `LlmClientVLLM` (vLLM HTTP-Client)
  - `LikertPromptFactory`, `AttributePromptFactory`
  - Prompt-Template-Generierung mit Persona-Context
  
- **Risiken:**
  - Falsche Prompts fÃ¼hren zu invaliden Antworten
  - Timeout bei langsamen Modellen
  - Connection-Pooling-Probleme bei hoher Concurrency
  - Scale-Reversal (in-order vs. reversed) inkorrekt implementiert
  - Rationale-Leakage (rationale erscheint wenn sie nicht sollte)
  
- **Test-Coverage:**
  - âœ… Unit: Prompt-Generierung fÃ¼r verschiedene Persona-Typen
  - âœ… Unit: Scale-Reversal-Logik (random50, in, rev modes)
  - âœ… Unit: Dual-Direction-Logic (dual_fraction)
  - âœ… Integration: Mock-vLLM-Server mit verschiedenen Response-Formaten
  - âœ… Integration: Timeout-Handling
  - âœ… Integration: Concurrent Request Handling

#### 2. **Output-Parsing & Validierung**
- **Komponenten:**
  - `LikertPostProcessor`, `AttributePostProcessor`
  - `AbstractPostProcessor` (JSON-Parsing, Sanitization)
  
- **Risiken:**
  - Modelle produzieren ungÃ¼ltiges JSON
  - Rating auÃŸerhalb 1-5 Range
  - Fehlende Required-Keys (name, appearance, biography)
  - Unerwartete Rationale bei `include_rationale=False`
  - Retry-Logic schlÃ¤gt endlos fehl
  
- **Test-Coverage:**
  - âœ… Unit: GÃ¼ltige JSON-Responses parsen
  - âœ… Unit: UngÃ¼ltige Formate (Markdown, Prosa, broken JSON)
  - âœ… Unit: Edge-Cases (Rating=0, Rating=6, Float-Ratings, String-Ratings)
  - âœ… Unit: Llama-Sanitization (<think>-Tags, doppelte Backticks)
  - âœ… Unit: Retry vs. Fail Decision-Logic
  - âœ… Unit: Rationale-Leak-Detection

#### 3. **Benchmark-Pipeline Orchestration**
- **Komponenten:**
  - `run_benchmark_pipeline()` in `benchmark.py`
  - `run_attr_gen_pipeline()` in `attr_gen.py`
  - Work-Item-Generierung (Persona Ã— Cases Cross-Join)
  - Completion-Tracking & Resume-Logic
  
- **Risiken:**
  - Duplicate Work-Items bei Resume
  - Falsche Persona-Count-Estimation
  - Memory-Overflow bei groÃŸen Datasets
  - Cancel-Signal wird nicht propagiert
  - Progress-Reporting inkorrekt
  
- **Test-Coverage:**
  - âœ… Unit: Work-Item-Generierung fÃ¼r kleines Dataset
  - âœ… Integration: Resume-Run Ã¼berspringt existierende Results
  - âœ… Integration: Cancel wÃ¤hrend Pipeline-Execution
  - âœ… Integration: Progress-Updates sind akkurat
  - âœ… Integration: Dual-Direction erstellt korrekte Anzahl Items

#### 4. **Queue-System & Task-Dependencies**
- **Komponenten:**
  - `QueueExecutor` in `executor.py`
  - Task-Dependency-Resolution
  - Background-Worker-Thread
  
- **Risiken:**
  - Deadlocks bei zirkulÃ¤ren Dependencies
  - Task bleibt in "running" nach Crash
  - Orphaned Tasks nach Restart
  - Race-Conditions bei Parallel-Execution
  - Status-Updates gehen verloren
  
- **Test-Coverage:**
  - âœ… Unit: Dependency-Graph-Resolution
  - âœ… Integration: Task-Chain-Execution (AttrGen â†’ Benchmark)
  - âœ… Integration: Orphan-Cleanup nach Restart
  - âœ… Integration: Pause/Resume wÃ¤hrend Task-Execution
  - âœ… Integration: Concurrent Task-Submission

### ðŸŸ¡ WICHTIG - Mittlere PrioritÃ¤t

#### 5. **Persona-Repositories & Data-Loading**
- **Komponenten:**
  - `FullPersonaRepository`, `FullPersonaRepositoryByDataset`
  - `PersonaRepository`, `PersonaRepositoryByDataset`
  - Attribute-Enrichment aus AttrGenerationRun
  
- **Risiken:**
  - Fehlende Personas bei Dataset-Switch
  - Attribute-Joins produzieren NULL-Werte
  - Count-Estimation vs. Actual-Count-Mismatch
  - Memory-Leak bei groÃŸen Datasets (kein Streaming)
  
- **Test-Coverage:**
  - âœ… Unit: Count-Estimation korrekt
  - âœ… Integration: Persona-Loading mit/ohne AttrGen-Filter
  - âœ… Integration: Dataset-Filtering funktioniert
  - âœ… Integration: Attribute-Enrichment korrekt gemappt

#### 6. **Analytics & Metrics-Berechnung**
- **Komponenten:**
  - `deltas_with_significance()` (Permutation-Tests)
  - `compute_order_effect_metrics()` (RMA, OBE)
  - `plot_fixed_effects_forest()` (Meta-Analysis)
  - Warm-Cache-System fÃ¼r Run-Detail-Page
  
- **Risiken:**
  - Statistische Berechnungen inkorrekt
  - NaN/Inf-Werte bei kleinen Samples
  - FDR-Korrektur (q-values) fehlerhaft
  - Warm-Cache schlÃ¤gt bei Missing-Data fehl
  - Order-Metrics bei fehlenden Dual-Directions
  
- **Test-Coverage:**
  - âœ… Unit: Delta-Berechnung mit bekannten Werten
  - âœ… Unit: Cliff's Delta berechnen
  - âœ… Unit: FDR-Korrektur (Benjamini-Hochberg)
  - âœ… Integration: Metrics mit realem Benchmark-Run
  - âœ… Integration: Warm-Cache-Pre-Computation

#### 7. **Database-Persistierung**
- **Komponenten:**
  - `BenchPersisterPeewee`, `PersisterPeewee`
  - Batch-Insert-Logic
  - Failure-Tracking
  
- **Risiken:**
  - Constraint-Violations (Unique-Constraints)
  - Transaction-Rollback verliert Daten
  - Batch-Insert schlÃ¤gt bei einem Item fehl â†’ ganze Batch verloren
  - Foreign-Key-Violations bei parallelen Deletes
  
- **Test-Coverage:**
  - âœ… Unit: Batch-Insert mit Duplicates
  - âœ… Integration: Concurrent Writes
  - âœ… Integration: Rollback-Behavior bei Errors
  - âœ… Integration: Failure-Tracking funktioniert

### ðŸŸ¢ WÃœNSCHENSWERT - Niedrige PrioritÃ¤t

#### 8. **API-Endpunkte**
- **Komponenten:**
  - `/benchmarks/start`, `/attrgen/start`
  - `/runs/{id}/metrics`, `/runs/{id}/deltas`
  - Resume-Logic Ã¼ber API
  
- **Test-Coverage:**
  - âœ… Integration: API-Start triggert Background-Task
  - âœ… Integration: Status-Polling wÃ¤hrend Run
  - âœ… Integration: Error-Responses bei Invalid-Params

#### 9. **Validatoren & Business-Rules**
- **Komponenten:**
  - `AttrGenValidator`, `DatasetValidator`
  
- **Test-Coverage:**
  - âœ… Unit: Deletion-Rules (running jobs, dependencies)
  - âœ… Unit: Resume-Validation

---

## Test-Architektur

### Unit-Tests
```
apps/backend/tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ benchmarking/
â”‚   â”‚   â”œâ”€â”€ test_prompt_factory.py          # Prompt-Generierung
â”‚   â”‚   â”œâ”€â”€ test_postprocessor_likert.py    # Output-Parsing
â”‚   â”‚   â”œâ”€â”€ test_postprocessor_attr.py
â”‚   â”‚   â”œâ”€â”€ test_scale_reversal.py          # Scale-Modes
â”‚   â”‚   â””â”€â”€ test_dual_direction.py
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ test_deltas.py                  # Statistik-Berechnungen
â”‚   â”‚   â”œâ”€â”€ test_order_metrics.py
â”‚   â”‚   â””â”€â”€ test_fdr_correction.py
â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”œâ”€â”€ test_persona_repository.py
â”‚   â”‚   â””â”€â”€ test_count_estimation.py
â”‚   â””â”€â”€ validators/
â”‚       â”œâ”€â”€ test_attrgen_validator.py
â”‚       â””â”€â”€ test_dataset_validator.py
```

### Integration-Tests
```
apps/backend/tests/
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_benchmark_pipeline_full.py     # âœ… Bereits vorhanden
â”‚   â”œâ”€â”€ test_attrgen_pipeline_full.py
â”‚   â”œâ”€â”€ test_queue_executor.py              # Task-Dependencies
â”‚   â”œâ”€â”€ test_resume_logic.py                # Resume-Runs
â”‚   â”œâ”€â”€ test_vllm_client.py                 # Mit Mock-vLLM-Server
â”‚   â”œâ”€â”€ test_warm_cache.py                  # Run-Detail-Seite
â”‚   â””â”€â”€ test_concurrent_writes.py           # DB-Race-Conditions
```

### End-to-End-Tests
```
apps/backend/tests/
â”œâ”€â”€ e2e/
â”‚   â”œâ”€â”€ test_full_workflow.py               # Dataset â†’ AttrGen â†’ Benchmark â†’ Analyse
â”‚   â”œâ”€â”€ test_api_workflow.py                # Via API-Endpunkte
â”‚   â””â”€â”€ test_cancel_resume.py               # Cancel + Resume-Szenarios
```

---

## Test-Fixtures & Mocking

### Mock-LLM-Client
```python
# tests/fixtures/mock_llm.py
class MockVLLMClient:
    """Simuliert vLLM-Responses ohne echte API-Calls."""
    
    def __init__(self, response_mode='valid'):
        self.response_mode = response_mode
        self.call_count = 0
        
    def run_stream(self, specs):
        for spec in specs:
            self.call_count += 1
            if self.response_mode == 'valid':
                yield self._valid_response(spec)
            elif self.response_mode == 'invalid_json':
                yield self._invalid_json(spec)
            elif self.response_mode == 'timeout':
                raise TimeoutError()
```

### Test-Datasets
```python
# tests/fixtures/test_data.py
def create_minimal_dataset():
    """2 Personas Ã— 3 Cases = 6 Items."""
    return {
        'personas': [...],
        'cases': [...]
    }

def create_large_dataset():
    """100 Personas Ã— 30 Cases = 3000 Items."""
    pass
```

### Mock-vLLM-Server
```python
# tests/fixtures/mock_vllm_server.py
import flask
app = Flask(__name__)

@app.route('/v1/completions', methods=['POST'])
def completions():
    """Fake vLLM /v1/completions endpoint."""
    prompt = request.json['prompt']
    if 'rating' in prompt.lower():
        return {'choices': [{'text': '{"rating": 3, "rationale": "ok"}'}]}
    else:
        return {'choices': [{'text': '{"name":"Max","appearance":"short","biography":"short"}'}]}
```

---

## Test-Daten-Management

### Isolation
- Jeder Test nutzt eigene SQLite-DB (`/tmp/test_benchmark_{test_id}.db`)
- Cleanup in `tearDown()` oder `pytest.fixture(scope='function')`

### Realistische Test-Daten
- **Small**: 2 Personas Ã— 3 Cases (Smoke-Tests)
- **Medium**: 10 Personas Ã— 10 Cases (Typical-Use-Case)
- **Large**: 100 Personas Ã— 30 Cases (Performance-Tests)

---

## Performance-Tests

### Benchmarks
```python
# tests/performance/test_benchmark_throughput.py
def test_benchmark_pipeline_throughput():
    """100 Personas Ã— 30 Cases in <5min mit fake-LLM."""
    pass

def test_vllm_concurrent_requests():
    """8 concurrent requests sÃ¤ttigen vLLM-Server."""
    pass
```

### Memory-Profile
```python
# tests/performance/test_memory_usage.py
@pytest.mark.memory
def test_large_dataset_memory():
    """1000 Personas sollten <500MB RAM nutzen."""
    pass
```

---

## CI/CD-Integration

### GitHub Actions Workflow
```yaml
name: Test Suite

on: [push, pull_request]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: pytest apps/backend/tests/unit/ -v
      
  integration-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: docker compose -f docker-compose.test.yml up -d
      - run: pytest apps/backend/tests/integration/ -v
      
  e2e-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: pytest apps/backend/tests/e2e/ -v --slow
```

### Pre-Commit Hook
```bash
#!/bin/bash
# .git/hooks/pre-commit
pytest apps/backend/tests/unit/ --maxfail=1 -q
```

---

## Test-Coverage-Ziele

| Komponente | Ziel | Aktuell |
|------------|------|---------|
| Prompt-Factories | 95% | - |
| PostProcessors | 95% | - |
| Benchmark-Pipeline | 90% | ~40% |
| AttrGen-Pipeline | 90% | - |
| Queue-Executor | 85% | - |
| Analytics | 80% | - |
| Repositories | 75% | - |
| API-Endpunkte | 70% | - |

**Gesamt-Ziel: 85% Coverage**

---

## Implementierungs-Reihenfolge

### Phase 1: Kritische Komponenten (Woche 1-2)
1. âœ… `test_prompt_factory.py` - Prompt-Generierung
2. âœ… `test_postprocessor_likert.py` - Output-Parsing
3. âœ… `test_scale_reversal.py` - Scale-Modes
4. âœ… `test_vllm_client.py` - LLM-Integration
5. âœ… `test_resume_logic.py` - Resume-Runs

### Phase 2: Pipeline & Queue (Woche 3)
6. âœ… `test_benchmark_pipeline_full.py` erweitern
7. âœ… `test_attrgen_pipeline_full.py`
8. âœ… `test_queue_executor.py`
9. âœ… `test_cancel_resume.py`

### Phase 3: Analytics & Performance (Woche 4)
10. âœ… `test_deltas.py` - Statistik
11. âœ… `test_warm_cache.py`
12. âœ… `test_benchmark_throughput.py`

### Phase 4: E2E & CI (Woche 5)
13. âœ… `test_full_workflow.py`
14. âœ… GitHub Actions Setup
15. âœ… Coverage-Reports

---

## Tools & Dependencies

```toml
# pyproject.toml oder requirements-test.txt
[tool.pytest.ini_options]
testpaths = ["apps/backend/tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: integration tests requiring DB",
    "e2e: end-to-end tests",
    "performance: performance benchmarks",
]

[tool.coverage.run]
source = ["apps/backend/src"]
omit = ["*/tests/*", "*/migrations/*"]

[tool.coverage.report]
precision = 2
show_missing = true
skip_covered = false
```

```bash
# requirements-test.txt
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-mock>=3.11.1
pytest-timeout>=2.1.0
pytest-xdist>=3.3.1  # Parallel test execution
faker>=19.0.0        # Test data generation
factory-boy>=3.3.0   # Model factories
responses>=0.23.1    # Mock HTTP requests
flask>=2.3.0         # Mock vLLM server
```

---

## Monitoring & Alerts

### Test-Metriken
- **Duration**: Jeder Test sollte <5s dauern (auÃŸer marked als `@pytest.mark.slow`)
- **Flakiness**: Max 1% Flaky-Rate
- **Coverage**: Mind. 85% maintained

### Pre-Production-Checks
```bash
# scripts/pre_production_checks.sh
#!/bin/bash
set -e

echo "Running full test suite..."
pytest apps/backend/tests/ -v --cov --cov-report=html

echo "Running static analysis..."
ruff check apps/backend/src/

echo "Running type checks..."
mypy apps/backend/src/

echo "âœ… All checks passed!"
```

---

## Zusammenfassung

**Kritischste Tests (Must-Have vor GPU-Run):**
1. âœ… Prompt-Generierung (Scale-Reversal, Dual-Direction)
2. âœ… Output-Parsing (alle Edge-Cases)
3. âœ… Resume-Logic (keine Duplicates)
4. âœ… vLLM-Integration (Timeout, Concurrency)
5. âœ… Queue-Executor (Dependencies, Cancellation)

**Aufwand-SchÃ¤tzung:**
- Unit-Tests: ~40 Stunden
- Integration-Tests: ~30 Stunden
- E2E-Tests: ~20 Stunden
- CI-Setup: ~10 Stunden
- **Gesamt: ~100 Stunden (2-3 Wochen Full-Time)**

**ROI:**
- Verhindert teure Fehler auf GPU-Servern
- Schnelleres Debugging durch isolierte Tests
- Confidence fÃ¼r refactorings
- Dokumentation durch Test-Cases
